-> Multiple Linear Regression
	- Multiple linear regression is a statistical technique used to model the relationship between a scalar response (or dependent variable) and one or more 
	  explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is
	  called multiple linear regression.
	- Multiple linear regression is a linear model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable 
	  (y). More specifically, that y can be calculated from a linear combination of the input variables (x).	










Ordinary Least Squares (OLS) Method : 
-----------------------------------
	- The most common method to estimate the coefficients is Ordinary Least Squares (OLS). OLS minimizes the sum of the residuals, the difference between the actual and predicted values of the dependent variable. 
	- The sum of the residuals is called the Residual Sum of Squares (RSS). The coefficients are chosen such that they minimize the RSS.
	- The OLS method is used to estimate the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. 
	- In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output (ŷ) over all samples in the dataset.
	- OLS can find the best parameters using of the following methods:
		1. Solving the model parameters analytically using closed-form equations
		2. Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)
	- The OLS method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0). 
	- The OLS method is used to estimate the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. 
	- In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output (ŷ) over all samples in the dataset.
	- OLS can find the best parameters using of the following methods:
		1. Solving the model parameters analytically using closed-form equations
		2. Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton’s Method, etc.)
	- The OLS method calculates the best-fitting line for the observed data by minimizing the sum of the squares of the vertical deviations from each data point to the line (if a point lies on the fitted line exactly, then its vertical deviation is 0).
	- The OLS method is used to estimate the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function.


	"The R-squared (R^2) value measures the proportion of the variance in the dependent variable that is predictable from the independent variables in a regression model. It ranges from 0 to 1.
	
	R^2 = 0 indicates that the model does not explain any of the variability of the response data around its mean.
	R^2 = 1 indicates that the model explains all the variability of the response data around its mean.
	
	In general, higher R^2 values indicate a better fit of the model to the data. However, the "best" R^2 value depends on various factors, and there is no universally agreed-upon threshold.
	
	Here are some general guidelines for interpreting R^2 values:
	
	R^2 < 0.3: Poor explanatory power
	0.3 ≤ R^2 < 0.5: Moderate explanatory power
	0.5 ≤ R^2 < 0.7: Good explanatory power
	R^2 ≥ 0.7: Excellent explanatory power
	
	However, R^2 should be interpreted in the context of the specific problem and the goals of the analysis. A high R^2 value does not necessarily mean that the model is useful for predictions."
	
